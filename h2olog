#!/usr/bin/env python
#
# h2olog - A BPF-backed request logging client for the H2O server.
#
# USAGE: $ sudo h2olog -p $(pgrep -o h2o)
#
# Copyright 2019 Fastly, Toru Maesaka

from __future__ import print_function
from bcc import BPF, USDT, __version__ as bcc_version
from collections import OrderedDict
import binascii, getopt, json, sys, time, os, platform

version = "0.0.1"

perf_ring_buffer_page_cnt = 256 # overridden by -P=N

# sudo itself if it is not running as root
if os.geteuid() != 0:
    os.execvp("sudo", ["-E", sys.executable] + sys.argv)


json_encoder = json.JSONEncoder(separators = (',', ':'))

default_bpf = """
#define MAX_STR_LEN 128

/*
 * "trace_line_t" is a general structure to store the data emitted by
 * USDT probes. This structure is pushed into the BPF ring buffer.
 */
struct trace_line_t {
    u64 conn_id;
    u64 req_id;
    u32 http_version;
    u32 http_status;
    u64 header_name_len;
    u64 header_value_len;
    char header_name[MAX_STR_LEN];
    char header_value[MAX_STR_LEN];
};

BPF_PERF_OUTPUT(rxbuf);
BPF_PERF_OUTPUT(txbuf);

int trace_receive_req(struct pt_regs *ctx) {
    struct trace_line_t line = {};

    bpf_usdt_readarg(1, ctx, &line.conn_id);
    bpf_usdt_readarg(2, ctx, &line.req_id);
    bpf_usdt_readarg(3, ctx, &line.http_version);

    if (rxbuf.perf_submit(ctx, &line, sizeof(line)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_receive_req_header(struct pt_regs *ctx) {
    struct trace_line_t line = {};
    void *pos = NULL;

    bpf_usdt_readarg(1, ctx, &line.conn_id);
    bpf_usdt_readarg(2, ctx, &line.req_id);

    // Extract the Header Name
    bpf_usdt_readarg(3, ctx, &pos);
    bpf_usdt_readarg(4, ctx, &line.header_name_len);
    bpf_probe_read(&line.header_name, sizeof(line.header_name), pos);

    // Extract the Header Value
    bpf_usdt_readarg(5, ctx, &pos);
    bpf_usdt_readarg(6, ctx, &line.header_value_len);
    bpf_probe_read(&line.header_value, sizeof(line.header_value), pos);

    if (rxbuf.perf_submit(ctx, &line, sizeof(line)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_send_resp(struct pt_regs *ctx) {
    struct trace_line_t line = {};

    bpf_usdt_readarg(1, ctx, &line.conn_id);
    bpf_usdt_readarg(2, ctx, &line.req_id);
    bpf_usdt_readarg(3, ctx, &line.http_status);

    if (txbuf.perf_submit(ctx, &line, sizeof(line)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}
"""

quic_bpf = """
#define MAX_STR_LEN 32
#define MAX_HEADER_VALUE_LEN 64
#define TOKEN_PREVIEW_LEN 8

#include <linux/kernel.h>

#define INIT_EVENT_NAME(event) do { \\
        sprintf((event).type, __func__ + (sizeof("trace_") -1)); \\
    } while (0)

struct st_quicly_stream_t {
    u64 dummy;
    s64 stream_id;
};

struct st_quicly_conn_t {
    u32 dummy[4];
    u32 master_id;
};

struct st_h2o_conn_t {
    void *dummy_ctx;
    void **dummy_hosts;
    u64 dummy_connected_at[2];
    u64 conn_id;
};

struct st_quicly_rtt_t {
    uint32_t min_rtt;
    uint32_t smoothed_rtt;
    uint32_t variance;
    uint32_t latest_rtt;
};

struct quic_event_t {
    char type[MAX_EVENT_TYPE_LEN];

    // from quicly
    char dcid[MAX_STR_LEN];
    char token_preview[TOKEN_PREVIEW_LEN];
    u64 time;
    u32 conn;
    u64 pn;
    u64 len;
    u8 packet_type;
    u8 first_octet;
    u64 frame_type;
    u32 ack_only;
    u32 newly_acked;
    u64 largest_acked;
    u32 bytes_acked;
    u32 ack_delay;
    u64 ack_block_begin;
    u64 ack_block_end;
    u64 max_lost_pn;
    u32 min_rtt;
    u32 smoothed_rtt;
    u32 latest_rtt;
    u32 cwnd;
    u32 inflight;
    u64 limit;
    u64 stream_id;
    u64 off;
    u32 is_unidirectional;
    u32 fin;
    u32 new_version;
    u64 token_generation;
    u64 error_code;
    u32 ret;

    // from h2o
    u64 h2o_conn;
    u64 h2o_req_id;
    char h2o_header_name[MAX_STR_LEN];
    char h2o_header_value[MAX_HEADER_VALUE_LEN];
};

// defining it to calculate the size of h2o specific data.
struct h2o_event_t {
    u64 h2o_conn;
    u64 h2o_req_id;
    char h2o_header_name[MAX_STR_LEN];
    char h2o_header_value[MAX_HEADER_VALUE_LEN];
};

BPF_PERF_OUTPUT(events);

int trace_quicly__accept(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &pos);
    bpf_probe_read(&event.dcid, MAX_STR_LEN, pos);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__receive(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &pos);
    bpf_probe_read(&event.dcid, MAX_STR_LEN, pos);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__version_switch(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.new_version);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__idle_timeout(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__stateless_reset_receive(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__crypto_decrypt(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.pn);
    bpf_usdt_readarg(4, ctx, &event.len);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__crypto_handshake(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.ret);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__packet_prepare(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.first_octet);
    bpf_usdt_readarg(4, ctx, &pos);
    bpf_probe_read(&event.dcid, MAX_STR_LEN, pos);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__packet_commit(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.pn);
    bpf_usdt_readarg(4, ctx, &event.len);
    bpf_usdt_readarg(5, ctx, &event.ack_only);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__packet_acked(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.pn);
    bpf_usdt_readarg(4, ctx, &event.newly_acked);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__packet_lost(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.pn);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__cc_ack_received(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.largest_acked);
    bpf_usdt_readarg(4, ctx, &event.bytes_acked);
    bpf_usdt_readarg(5, ctx, &event.cwnd);
    bpf_usdt_readarg(6, ctx, &event.inflight);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__cc_congestion(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.max_lost_pn);
    bpf_usdt_readarg(4, ctx, &event.inflight);
    bpf_usdt_readarg(5, ctx, &event.cwnd);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__quictrace_cc_ack(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    struct st_quicly_rtt_t rtt = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &pos);
    bpf_probe_read(&rtt, sizeof(rtt), pos);
    event.min_rtt = rtt.min_rtt;
    event.smoothed_rtt = rtt.smoothed_rtt;
    event.latest_rtt = rtt.latest_rtt;
    bpf_usdt_readarg(4, ctx, &event.cwnd);
    bpf_usdt_readarg(5, ctx, &event.inflight);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}


int trace_quicly__quictrace_cc_lost(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    struct st_quicly_rtt_t rtt = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &pos);
    bpf_probe_read(&rtt, sizeof(rtt), pos);
    event.min_rtt = rtt.min_rtt;
    event.smoothed_rtt = rtt.smoothed_rtt;
    event.latest_rtt = rtt.latest_rtt;
    bpf_usdt_readarg(4, ctx, &event.cwnd);
    bpf_usdt_readarg(5, ctx, &event.inflight);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__transport_close_send(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.frame_type);

    return 0;
}

int trace_quicly__transport_close_receive(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.error_code);
    bpf_usdt_readarg(4, ctx, &event.frame_type);

    return 0;
}

int trace_quicly__application_close_send(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.error_code);

    return 0;
}

int trace_quicly__new_token_send(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &pos);
    bpf_probe_read(&event.token_preview, TOKEN_PREVIEW_LEN, pos);
    bpf_usdt_readarg(4, ctx, &event.len);
    bpf_usdt_readarg(5, ctx, &event.token_generation);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__new_token_acked(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.token_generation);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__new_token_receive(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &pos);
    bpf_probe_read(&event.token_preview, TOKEN_PREVIEW_LEN, pos);
    bpf_usdt_readarg(4, ctx, &event.len);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__streams_blocked_send(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.limit);
    bpf_usdt_readarg(4, ctx, &event.is_unidirectional);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__streams_blocked_receive(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.limit);
    bpf_usdt_readarg(4, ctx, &event.is_unidirectional);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__data_blocked_receive(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.off);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__stream_data_blocked_receive(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.stream_id);
    bpf_usdt_readarg(4, ctx, &event.limit);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__quictrace_sent(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.pn);
    bpf_usdt_readarg(4, ctx, &event.len);
    bpf_usdt_readarg(5, ctx, &event.packet_type);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__quictrace_send_stream(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    struct st_quicly_stream_t stream = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &pos);
    bpf_probe_read(&stream, sizeof(stream), pos);
    if (stream.stream_id < 0)
        event.stream_id = stream.stream_id + 31337;
    else
        event.stream_id = stream.stream_id;
    bpf_usdt_readarg(4, ctx, &event.off);
    bpf_usdt_readarg(5, ctx, &event.len);
    bpf_usdt_readarg(6, ctx, &event.fin);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__quictrace_recv(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.pn);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__quictrace_recv_ack(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.ack_block_begin);
    bpf_usdt_readarg(4, ctx, &event.ack_block_end);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__quictrace_recv_ack_delay(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.ack_delay);

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_quicly__quictrace_lost(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;
    bpf_usdt_readarg(2, ctx, &event.time);
    bpf_usdt_readarg(3, ctx, &event.pn );

    if (events.perf_submit(ctx, &event, sizeof(event) - sizeof(struct h2o_event_t)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

#ifdef ENABLE_H2O_PROBES
BPF_HASH(h2o_to_quicly_conn, u64, u32);

int trace_h2o__send_response_header(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    size_t name_len = 0;
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &event.h2o_conn);

    const u32 *conn_ptr = h2o_to_quicly_conn.lookup(&event.h2o_conn);
    if (conn_ptr == NULL)
        return 0;
    event.conn = *conn_ptr;

    bpf_usdt_readarg(2, ctx, &event.h2o_req_id);
    bpf_usdt_readarg(3, ctx, &pos);
    bpf_usdt_readarg(4, ctx, &name_len);
    bpf_probe_read(&event.h2o_header_name, MAX_STR_LEN, pos);

#ifdef CHECK_ALLOWED_RES_HEADER_NAME
    if (!CHECK_ALLOWED_RES_HEADER_NAME(event.h2o_header_name, name_len)) {
        return 0;
    }
#endif

    bpf_usdt_readarg(5, ctx, &pos);
    // ignore arg 6 (value_len)
    bpf_probe_read(&event.h2o_header_value, MAX_HEADER_VALUE_LEN, pos);

    if (events.perf_submit(ctx, &event, sizeof(event)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_h2o__h3_accept(struct pt_regs *ctx) {
    void *pos = NULL;
    struct quic_event_t event = {};
    struct st_quicly_conn_t conn = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &event.h2o_conn);
    // ignore arg 2 (struct st_h2o_conn_t*)
    bpf_usdt_readarg(3, ctx, &pos);
    bpf_probe_read(&conn, sizeof(conn), pos);
    event.conn = conn.master_id;

    h2o_to_quicly_conn.update(&event.h2o_conn, &event.conn);

    if (events.perf_submit(ctx, &event, sizeof(event)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}

int trace_h2o__h3_close(struct pt_regs *ctx) {
    struct quic_event_t event = {};
    INIT_EVENT_NAME(event);

    bpf_usdt_readarg(1, ctx, &event.h2o_conn);

    const u32 *conn_ptr = h2o_to_quicly_conn.lookup(&event.h2o_conn);
    if (conn_ptr != NULL) {
        event.conn = *conn_ptr;
    } else {
        bpf_trace_printk("h2o_conn=%lu is not associated to conn\\n", event.h2o_conn);
    }
    h2o_to_quicly_conn.delete(&event.h2o_conn);

    if (events.perf_submit(ctx, &event, sizeof(event)) < 0)
        bpf_trace_printk("failed to perf_submit\\n");

    return 0;
}
#endif /* ENABLE_H2O_PROBES */
"""

quic_type_map =  {
    "quicly__accept": ["dcid"],
    "quicly__receive": ["dcid"],
    "quicly__version_switch": ["new_version"],
    "quicly__crypto_decrypt": ["pn", "len"],
    "quicly__crypto_handshake": ["ret"],
    "quicly__packet_prepare": ["first_octet", "dcid"],
    "quicly__packet_commit": ["pn", "len", "ack_only"],
    "quicly__packet_acked": ["pn", "newly_acked"],
    "quicly__packet_lost": ["pn"],
    "quicly__cc_ack_received": ["largest_acked", "bytes_acked", "cwnd", "inflight"],
    "quicly__cc_congestion": ["max_lost_pn", "inflight", "cwnd"],
    "quicly__quictrace_recv_ack": ["ack_block_begin", "ack_block_end"],
    "quicly__quictrace_cc_lost": ["min_rtt", "smoothed_rtt", "latest_rtt", "cwnd", "inflight"],
    "quicly__quictrace_cc_ack": ["min_rtt", "smoothed_rtt", "latest_rtt", "cwnd", "inflight"],
    "quicly__transport_close_send": ["frame_type"],
    "quicly__transport_close_receive": ["error_code", "frame_type"],
    "quicly__application_close_send": ["error_code"],
    "quicly__new_token_send": ["token_preview", "len", "token_generation"],
    "quicly__new_token_acked": ["token_generation"],
    "quicly__streams_blocked_send": ["limit", "is_unidirectional"],
    "quicly__streams_blocked_receive": ["limit", "is_unidirectional"],
    "quicly__data_blocked_receive": ["off"],
    "quicly__stream_data_blocked_receive": ["stream_id", "limit"],
    "quicly__quictrace_sent": ["pn", "len", "packet_type"],
    "quicly__quictrace_send_stream": ["stream_id", "off", "len", "fin"],
    "quicly__quictrace_recv": ["pn"],
    "quicly__quictrace_recv_ack_delay": ["ack_delay"],
    "quicly__quictrace_lost": ["pn"],
    "h2o__send_response_header": ["h2o_conn", "h2o_req_id", "h2o_header_name", "h2o_header_value"],
    "h2o__h3_accept": ["h2o_conn"],
    "h2o__h3_close": ["h2o_conn"],
}

# Hack to make it work both on python2 and python3
if sys.version_info.major >= 3:
    def s(thingy):
        if isinstance(thingy, bytes):
            return thingy.decode("utf-8")
        else:
            return thingy
else:
    def s(thingy):
        return thingy


def handle_req_line(cpu, data, size):
    line = b["rxbuf"].event(data)
    if line.http_version:
        v = "HTTP/%d.%d" % (line.http_version / 256, line.http_version % 256)
        print("%u %u RxProtocol %s" % (line.conn_id, line.req_id, v))
    else:
        hn = line.header_name[:line.header_name_len]
        hv = line.header_value[:line.header_value_len]
        print("%u %u RxHeader   %s %s" % (line.conn_id, line.req_id, s(hn), s(hv)))

def handle_resp_line(cpu, data, size):
    line = b["txbuf"].event(data)
    print("%u %u TxStatus   %d" % (line.conn_id, line.req_id, line.http_status))

def load_common_fields(hsh, line):
    hsh["time"] = int(time.time() * 1000) if line.time == 0 else line.time
    hsh["type"] = line.type.replace("quicly__", "", 1).replace("_", "-")
    hsh["conn"] = line.conn

def build_quic_trace_result(res, event, fields):
    for k in fields:
        outk = k.replace("_", "-")
        res[outk] = s(getattr(event, k))
        if k == "token_preview":
            res[outk] = binascii.hexlify(res[outk])

def handle_quic_event(cpu, data, size):
    ev = b["events"].event(data)
    if len(allowed_quic_event) > 0 and ev.type not in allowed_quic_event:
        return

    res = OrderedDict()
    load_common_fields(res, ev)
    build_quic_trace_result(res, ev, quic_type_map.get(ev.type, []))
    print(json_encoder.encode(res))

def usage():
    print("""
USAGE: h2olog -p PID
       h2olog quic -p PID
       h2olog quic -t event_type -p PID
       h2olog quic -v -s response_header_name -p PID

Other options:
    -P [int] The size of the perf ring buffer. Default to 256.
    -h Shows this help and exit.
    -d Shows the BPF program and exit.

h2olog version %s
Platform: %s
BCC: %s
""".strip() % (version, platform.platform(), bcc_version))
    exit()

def trace_http():
    b["rxbuf"].open_perf_buffer(handle_req_line, page_cnt=perf_ring_buffer_page_cnt)
    b["txbuf"].open_perf_buffer(handle_resp_line, page_cnt=perf_ring_buffer_page_cnt)

    while 1:
        try:
            b.perf_buffer_poll()
        except KeyboardInterrupt:
            exit()

def trace_quic():
    b["events"].open_perf_buffer(handle_quic_event, page_cnt=perf_ring_buffer_page_cnt)
    while 1:
        try:
            b.perf_buffer_poll()
        except KeyboardInterrupt:
            exit()

def get_event_type_len(usdt, providers):
    providers_set = set(providers)
    probes = filter(lambda probe: probe.provider in providers_set, usdt.enumerate_probes())
    return 1 + max(map(lambda probe: len("%s__%s" % (probe.provider, probe.name)), probes))

def generate_filter_cflag(candidates):
    conditions = []
    for candidate in candidates:
        exprs = ["/* %s */ (l) == %d" % (candidate, len(candidate))]
        for i, c in enumerate(candidate):
            exprs.append("(s)[%d] == '%s'" % (i, c))
        conditions.append("(%s)" % " && ".join(exprs))

    return "-DCHECK_ALLOWED_RES_HEADER_NAME(s,l)=(%s)" % " || ".join(conditions)

def is_power_of_two(n):
    return (n > 0) and (n & (n-1) == 0)

def is_valid_page_cnt(s):
    try:
        n = int(s)
        return is_power_of_two(n)
    except:
        return False

if len(sys.argv) < 1:
    usage()

tracer_func = trace_http
optidx = 1
if len(sys.argv) > 1 and sys.argv[1] == "quic":
    tracer_func = trace_quic
    optidx = 2

try:
    h2o_pid = 0
    allowed_quic_event = set()
    allowed_res_header_name = set()
    verbose = False
    debug = False
    opts, args = getopt.getopt(sys.argv[optidx:], 'hvp:t:s:dP:')
    for opt, arg in opts:
        if opt == "-h":
            usage()
        elif opt == "-p":
            h2o_pid = arg
        elif opt == "-t":
            allowed_quic_event.update(arg.replace(":", "__", 1).split(","))
        elif opt == "-v":
            verbose = True
        elif opt == "-s": # reSponse
            allowed_res_header_name.update(arg.lower().split(","))
        elif opt == "-d": # Debug
            debug = True
        elif opt == "-P":
            if not is_valid_page_cnt(arg):
                print("Invalid value for -P, which must be a power of two: %s" % arg, file=sys.stderr)
                sys.exit(1)
            perf_ring_buffer_page_cnt = int(arg)


except getopt.error as msg:
    print(msg)
    sys.exit(2)

if h2o_pid == 0:
    usage()

u = USDT(pid=int(h2o_pid))

if sys.argv[1] == "quic":
    # provider qiucly:
    u.enable_probe(probe="accept", fn_name="trace_quicly__accept")
    u.enable_probe(probe="receive", fn_name="trace_quicly__receive")
    u.enable_probe(probe="version_switch", fn_name="trace_quicly__version_switch")
    u.enable_probe(probe="idle_timeout", fn_name="trace_quicly__idle_timeout")
    u.enable_probe(probe="stateless_reset_receive", fn_name="trace_quicly__stateless_reset_receive")
    u.enable_probe(probe="crypto_decrypt", fn_name="trace_quicly__crypto_decrypt")
    u.enable_probe(probe="crypto_handshake", fn_name="trace_quicly__crypto_handshake")
    u.enable_probe(probe="packet_prepare", fn_name="trace_quicly__packet_prepare")
    u.enable_probe(probe="packet_commit", fn_name="trace_quicly__packet_commit")
    u.enable_probe(probe="packet_acked", fn_name="trace_quicly__packet_acked")
    u.enable_probe(probe="packet_lost", fn_name="trace_quicly__packet_lost")
    u.enable_probe(probe="cc_ack_received", fn_name="trace_quicly__cc_ack_received")
    u.enable_probe(probe="cc_congestion", fn_name="trace_quicly__cc_congestion")
    u.enable_probe(probe="transport_close_send", fn_name="trace_quicly__transport_close_send")
    u.enable_probe(probe="transport_close_receive", fn_name="trace_quicly__transport_close_receive")
    u.enable_probe(probe="application_close_send", fn_name="trace_quicly__application_close_send")
    u.enable_probe(probe="new_token_send", fn_name="trace_quicly__new_token_send")
    u.enable_probe(probe="new_token_acked", fn_name="trace_quicly__new_token_acked")
    u.enable_probe(probe="new_token_receive", fn_name="trace_quicly__new_token_receive")
    u.enable_probe(probe="streams_blocked_send", fn_name="trace_quicly__streams_blocked_send")
    u.enable_probe(probe="streams_blocked_receive", fn_name="trace_quicly__streams_blocked_receive")
    u.enable_probe(probe="data_blocked_receive", fn_name="trace_quicly__data_blocked_receive")
    u.enable_probe(probe="stream_data_blocked_receive", fn_name="trace_quicly__stream_data_blocked_receive")
    u.enable_probe(probe="quictrace_sent", fn_name="trace_quicly__quictrace_sent")
    u.enable_probe(probe="quictrace_send_stream", fn_name="trace_quicly__quictrace_send_stream")
    u.enable_probe(probe="quictrace_recv", fn_name="trace_quicly__quictrace_recv")
    u.enable_probe(probe="quictrace_recv_ack", fn_name="trace_quicly__quictrace_recv_ack")
    u.enable_probe(probe="quictrace_recv_ack_delay", fn_name="trace_quicly__quictrace_recv_ack_delay")
    u.enable_probe(probe="quictrace_lost", fn_name="trace_quicly__quictrace_lost")
    u.enable_probe(probe="quictrace_cc_ack", fn_name="trace_quicly__quictrace_cc_ack")
    u.enable_probe(probe="quictrace_cc_lost", fn_name="trace_quicly__quictrace_cc_lost")

    cflags = ["-DMAX_EVENT_TYPE_LEN=%s" % get_event_type_len(u, ["h2o", "quicly"])]

    # provider h2o:
    if verbose:
        u.enable_probe(probe="send_response_header", fn_name="trace_h2o__send_response_header")
        u.enable_probe(probe="h3_accept", fn_name="trace_h2o__h3_accept")
        u.enable_probe(probe="h3_close", fn_name="trace_h2o__h3_close")
        cflags.append("-DENABLE_H2O_PROBES")
        if len(allowed_res_header_name):
            cflags.append(generate_filter_cflag(allowed_res_header_name))

    bpf = quic_bpf
else:
    u.enable_probe(probe="receive_request", fn_name="trace_receive_req")
    u.enable_probe(probe="receive_request_header", fn_name="trace_receive_req_header")
    u.enable_probe(probe="send_response", fn_name="trace_send_resp")
    cflags = []
    bpf = default_bpf

if debug:
    print("/* cflags = %s */" % " ".join(cflags))
    print(bpf)
    sys.exit()

b = BPF(text=bpf, usdt_contexts=[u], cflags=cflags)

tracer_func()
